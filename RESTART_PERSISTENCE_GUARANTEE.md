# ğŸ”„ **RESTART PERSISTENCE GUARANTEE**

## âœ… **GUARANTEED: Everything Persists After Restart**

This document explains **EXACTLY** what happens when you:
```bash
docker compose down
docker compose up -d
```

---

## ğŸ“Š **DATA PERSISTENCE MATRIX**

| Component | Storage Location | Persists? | Volume Mount | Notes |
|-----------|-----------------|-----------|--------------|-------|
| **PostgreSQL Database** | `/var/lib/postgresql/data` | âœ… YES | `postgres-data` | All tables & data persist |
| **RabbitMQ Queues** | `/var/lib/rabbitmq` | âœ… YES | `rabbitmq-data` | Queues, bindings, exchanges persist |
| **Redis Cache** | `/data` | âœ… YES | `redis-data` | Cache entries persist (AOF enabled) |
| **MinIO Files** | `/data` | âœ… YES | `minio-data` | Uploaded scenario files persist |
| **PgAdmin Config** | `/var/lib/pgadmin` | âœ… YES | `pgadmin-data` | Server connections persist |
| **Prometheus Metrics** | `/prometheus` | âœ… YES | `prometheus-data` | Historical metrics persist |
| **Grafana Dashboards** | `/var/lib/grafana` | âœ… YES | `grafana-data` | Dashboards & settings persist |

---

## ğŸ—„ï¸ **POSTGRESQL DATABASE - WHAT PERSISTS**

### **After Restart, You Will See:**

#### âœ… **All Tables (Auto-created by JPA/Hibernate on first run):**
```sql
-- Scenario Library Service Tables
scenarios
tracks
simulations
scenario_files
track_segments

-- Webhook Management Service Tables
webhooks
webhook_headers
webhook_deliveries
webhook_delivery_attempts

-- Other Tables
schema_version (Flyway migrations)
```

#### âœ… **All Data Records:**
- **16 scenarios** (demo scenarios from seed data)
- **3+ active webhook subscriptions**
- **All webhook delivery records**
- **All tracks, simulations, and their relationships**

### **What You DON'T Need to Do:**
- âŒ Recreate tables (JPA creates them automatically on first startup)
- âŒ Re-seed scenarios (they're already in the database)
- âŒ Re-create webhook subscriptions
- âŒ Re-run database migrations

### **Verification After Restart:**
```bash
# Check database contents
docker exec -it postgres psql -U postgres -d postgres -c "\dt"
docker exec -it postgres psql -U postgres -d postgres -c "SELECT COUNT(*) FROM scenarios;"
docker exec -it postgres psql -U postgres -d postgres -c "SELECT COUNT(*) FROM webhooks;"
docker exec -it postgres psql -U postgres -d postgres -c "SELECT COUNT(*) FROM webhook_deliveries;"
```

---

## ğŸ° **RABBITMQ QUEUES - WHAT PERSISTS**

### **After Restart, You Will See:**

#### âœ… **All Queues:**
```
webhook.events              (Main queue for webhook events)
webhook.events.dlq          (Dead Letter Queue)
simulation.events           (If used)
track.events               (If used)
```

#### âœ… **All Exchanges:**
```
webhook.exchange           (Direct exchange for webhook routing)
amq.direct                 (Default exchange)
```

#### âœ… **All Bindings:**
```
webhook.exchange â†’ webhook.events (routing key: webhook.events)
```

#### âœ… **All Messages:**
- Messages in queues persist (RabbitMQ uses disk storage)
- DLQ messages persist (need manual purging if undesired)

### **What You DON'T Need to Do:**
- âŒ Recreate queues (RabbitMQ restores them from `/var/lib/rabbitmq`)
- âŒ Recreate exchanges
- âŒ Recreate bindings
- âŒ Re-configure RabbitMQ users/permissions

### **Verification After Restart:**
```bash
# Check RabbitMQ queues
curl -u admin:admin123 http://localhost:15672/api/queues | jq '.[].name'

# Check message counts
curl -u admin:admin123 http://localhost:15672/api/queues/%2F/webhook.events | jq '{messages, consumers}'
```

---

## ğŸ“¦ **MINIO FILES - WHAT PERSISTS**

### **After Restart, You Will See:**

#### âœ… **All Buckets:**
```
dco-scenario-library-service    (Main bucket for scenario files)
```

#### âœ… **All Uploaded Files:**
- Scenario definition files
- Software packages
- Track data files
- Any other uploaded artifacts

### **What You DON'T Need to Do:**
- âŒ Recreate buckets
- âŒ Re-upload files

### **Verification After Restart:**
```bash
# Check MinIO buckets and files
docker exec -it minio mc ls local/dco-scenario-library-service/
```

---

## ğŸ”„ **RESTART SCENARIOS**

### **Scenario 1: Normal Restart (Data Persists)**
```bash
# Stop all services
docker compose down

# Start all services
docker compose up -d

# âœ… Result: All data, queues, files persist!
```

### **Scenario 2: Rebuild Services (Data Persists)**
```bash
# Rebuild and restart services
docker compose up -d --build

# âœ… Result: All data, queues, files persist!
# Note: Code changes applied, but data remains intact
```

### **Scenario 3: Nuclear Option (Data LOST)**
```bash
# DANGER: This deletes ALL data!
docker compose down -v

# âŒ Result: All volumes deleted, data lost!
# You'll need to re-seed scenarios and webhooks
```

---

## ğŸš€ **STARTUP SEQUENCE (Automatic)**

The `docker-compose.yml` has dependency management:

```yaml
1. postgres      â†’ Starts first (healthcheck: pg_isready)
2. rabbitmq      â†’ Starts second (healthcheck: rabbitmq-diagnostics)
3. redis         â†’ Starts third
4. minio         â†’ Starts fourth
5. pgadmin       â†’ Depends on postgres

6. message-queue-service        â†’ Depends on rabbitmq (healthy)
7. webhook-management-service   â†’ Depends on postgres + rabbitmq (healthy)
8. scenario-library-service     â†’ Depends on postgres + message-queue-service
9. tracks-management-service    â†’ Depends on postgres
10. dco-gateway                 â†’ Depends on redis
11. developer-console-ui        â†’ Depends on dco-gateway
```

**All services will wait for dependencies before starting.**

---

## ğŸ§ª **POST-RESTART VERIFICATION CHECKLIST**

### **1. Check All Services Are Running:**
```bash
docker compose ps
```

**Expected Output:**
```
âœ… All services: Up (healthy)
```

### **2. Check Database Data:**
```bash
# Check scenarios
curl http://localhost:8082/scenarios | jq length
# Expected: 16

# Check webhooks
curl http://localhost:8084/webhooks | jq length
# Expected: 3+

# Check deliveries
docker exec -it postgres psql -U postgres -c "SELECT COUNT(*) FROM webhook_deliveries;"
# Expected: Previous delivery count
```

### **3. Check RabbitMQ Queues:**
```bash
curl -u admin:admin123 http://localhost:15672/api/queues | jq '.[].name'
```

**Expected:**
```json
[
  "webhook.events",
  "webhook.events.dlq"
]
```

### **4. Check MinIO Files:**
```bash
docker exec -it minio mc ls local/dco-scenario-library-service/
```

**Expected:** All previously uploaded files

---

## ğŸ“‹ **COMMON QUESTIONS**

### **Q1: Do I need to run seed scripts after restart?**
âŒ **NO!** All data persists in PostgreSQL volume.

### **Q2: Will RabbitMQ queues be recreated automatically?**
âœ… **YES!** RabbitMQ restores queues from `/var/lib/rabbitmq` volume.

### **Q3: Will webhook subscriptions disappear?**
âŒ **NO!** They're stored in PostgreSQL and persist.

### **Q4: Will delivery history be lost?**
âŒ **NO!** All `webhook_deliveries` records persist in PostgreSQL.

### **Q5: What if I rebuild a service (docker compose up --build)?**
âœ… **Data persists!** Volumes are independent of container images.

### **Q6: What if I want to start fresh?**
```bash
# Delete all volumes and data
docker compose down -v

# Rebuild and restart
docker compose up -d --build

# Re-seed scenarios (if needed)
./seed-database.sh

# Re-create webhooks (if needed)
./seed-test-webhook.sh
```

---

## âš ï¸ **ONLY WAY TO LOSE DATA**

You will **ONLY** lose data if you:

1. **Run `docker compose down -v`** (deletes volumes)
2. **Manually delete volumes:**
   ```bash
   docker volume rm eclipse_sdv_devloper_console-_postgres-data
   docker volume rm eclipse_sdv_devloper_console-_rabbitmq-data
   ```
3. **Delete the entire Docker data directory**

---

## ğŸ¯ **PRODUCTION DEPLOYMENT NOTES**

### **Backup Strategy:**
```bash
# Backup PostgreSQL
docker exec -it postgres pg_dump -U postgres postgres > backup.sql

# Backup volumes
docker run --rm \
  -v eclipse_sdv_devloper_console-_postgres-data:/data \
  -v $(pwd):/backup \
  alpine tar czf /backup/postgres-backup.tar.gz /data
```

### **Restore Strategy:**
```bash
# Restore PostgreSQL
cat backup.sql | docker exec -i postgres psql -U postgres postgres

# Restore volumes
docker run --rm \
  -v eclipse_sdv_devloper_console-_postgres-data:/data \
  -v $(pwd):/backup \
  alpine tar xzf /backup/postgres-backup.tar.gz -C /
```

---

## âœ… **FINAL ANSWER**

### **After `docker compose down && docker compose up -d`:**

| Data Type | Status | Location |
|-----------|--------|----------|
| **Scenarios (16)** | âœ… Persists | PostgreSQL â†’ `postgres-data` volume |
| **Tracks** | âœ… Persists | PostgreSQL â†’ `postgres-data` volume |
| **Simulations** | âœ… Persists | PostgreSQL â†’ `postgres-data` volume |
| **Webhooks (3+)** | âœ… Persists | PostgreSQL â†’ `postgres-data` volume |
| **Webhook Deliveries** | âœ… Persists | PostgreSQL â†’ `postgres-data` volume |
| **RabbitMQ Queues** | âœ… Persists | RabbitMQ â†’ `rabbitmq-data` volume |
| **Queue Bindings** | âœ… Persists | RabbitMQ â†’ `rabbitmq-data` volume |
| **Messages in Queues** | âœ… Persists | RabbitMQ â†’ `rabbitmq-data` volume |
| **MinIO Files** | âœ… Persists | MinIO â†’ `minio-data` volume |
| **Redis Cache** | âœ… Persists | Redis â†’ `redis-data` volume |

### **YOU NEED TO DO:**
1. âœ… `docker compose up -d` (that's it!)

### **YOU DON'T NEED TO DO:**
- âŒ Recreate database tables
- âŒ Re-seed scenarios
- âŒ Re-create webhooks
- âŒ Recreate RabbitMQ queues
- âŒ Re-upload files to MinIO
- âŒ Any manual configuration

---

## ğŸ‰ **GUARANTEE**

**Your system is production-ready with full data persistence!**

Every restart will:
- âœ… Preserve all database records
- âœ… Preserve all RabbitMQ queues and messages
- âœ… Preserve all uploaded files
- âœ… Restore all service configurations
- âœ… Maintain all webhooks and delivery history

**No manual intervention required!**
